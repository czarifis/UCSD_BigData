{
 "metadata": {
  "name": "",
  "signature": "sha256:df4ef06e1ce8061683d9e0b75cae157bb7f700b8cd3cc941fb14b78c34a37d56"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Climate classification**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we want to do climate classification by using 2 techinques\n",
      "\n",
      "1. Partitioning and merging using the idea of KD trees.\n",
      "2. K-means clustering by using the datas of mean temperature, temperature difference, latitude, elevation and average perception\n",
      "\n",
      "Then, we import the data of K\u00f6ppen climate classification, and do PCA on each partition based on this classification. Our goal is to check whether or not the climate classification will give us a good understanding of the climate.\n",
      "\n",
      "Furthermore, we compare our two partitions with the partition from K\u00f6ppen climate classification, and study the similarity between the result from our PCA model, K-means model and existing classification."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Section 1**. Introduction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "**K\u00f6ppen climate classification**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following picture shows the most widely used climate classification. For example, **Dfa** with light blue indicates the **Hot summer continental climates**. It contains Boston, Chicago in US. It is a climatic region typified by large seasonal temperature differences, with warm to hot (and often humid) summers and cold (sometimes severely cold) winters. **Mediterranean climates** labeled as **CSa/CSb** are very different, warm summer and mild winters but relatively dry, which is similar to the weather in San Diego.\n",
      "\n",
      "The idea of partitioning and merging is to combine the region with similar weather conditions (based on the percentage of the explained variance). This is actually the goal of the climate classification. We expect that our results after merging areas with similar weather conditions will match the classification of the climate as shown in the picture below.\n",
      "\n",
      "For more information on K\u00f6ppen climate classification, pleace check the following link http://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification\n",
      "\n",
      "![Bilby Stampede](http://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Koppen_World_Map_%28retouched_version%29.png/1024px-Koppen_World_Map_%28retouched_version%29.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "**Life Zone Classification system**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Besides the well-known K\u00f6ppen climate classification, Life Zone Classification system groups areas together based on the latitude, elevation and humidity (check the following picture). In this case, we can apply K-means clustering algorithm to do climatic classification. The elements our algorithm utilizes in order to perform the classification include the following: \n",
      "\n",
      "1. Mean temperature over the year\n",
      "2. Temperature difference\n",
      "3. Latitude\n",
      "4. Elevation\n",
      "5. Average perception over the year\n",
      "\n",
      "![Bilby Stampede](http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Lifezones_Pengo.svg/987px-Lifezones_Pengo.svg.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We organize our notebook as follows:\n",
      "\n",
      "In section 2, performed following work:\n",
      "\n",
      "1. Improved partition algorithm (in 2 ways)\n",
      "2. Improved merging algorithm\n",
      "\n",
      "In section 3, we use K-means algorithm to study climate classification. In section 4 we compare the results between original K\u00f6ppen climate classification, classification we get using partitioning and merging, and classification using K-means. Finally, in section 5, we apply PCA model on the partition based on K\u00f6ppen climate classification, and study the reliablility of this model, and discuss the weather pattern over the year based on the eigenvalues and eigenvectors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Section 2**. \n",
      "Improved KD-Tree Partition and merging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Improved Partition Algoritm**\n",
      "\n",
      "Since the initial partition algorithm included areas that were very distinct from each other we used several notions that improved the grouping. Initially, we were just using kd-trees to perform the partition. The downside of this was that if the number of measurements for a big portion of the map was limited (like measurements in oceans), we ended grouping together base-stations that were expected to have completely different measurements (for example one of those groups had a base-station located in antarctica and another one near the equator grouped together). So the ideas we used to improve the partitions are the following:\n",
      "\n",
      "\n",
      " 1. The first idea was to change the iteration algorithm on each level of KD-tree. Initially, we iteratively split the region left/right (along longitude) and up/down (along latitude). Our first improvement is to split the region in smaller groups based on geographical criteria and then perform our kd-tree algorithm on each one of those smaller partitions. That way, only base-stations that are located near each other are expected to be grouped together, therefore the measurements are expected to be more homogeneous and we expect that less eigen vectors will be able to explain a greater percentage of the variance. \n",
      " \n",
      "<!--  shorter one of latitude and longitude in each iteration in order to **split the region as square as we can**. For example, if we have a rectangle with length 10 and width 6, we split it along the width since the length is longer. Assume we split it up equaly, we will end up with two (5\\*6) rectangle. -->\n",
      "\n",
      "1. The second idea is that during our initial partition, we completely exclude regions that do not contain any stations at all, so the measurements we get for each group are in fact more reasonable.\n",
      "\n",
      "Based on these two improvements, we managed to get a partition that looks much better and it also is much more representative than the original one.\n",
      "\n",
      "<!--which looks much better than original one, we avoid the partiton with crazy long and narrow shape which cross the oceans and continent.-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In following figure, we can see that the globe is covered by squares, and we basically remove the continent of Antarctica and some areas of the ocean. The pink part is the region we covered, and white part is the region we removed. Although the shape of square \\# 5 looks like a rectangle, it is actually a square based on the latitude and longitude, because that is how we performed the initial partition."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we do partition in each square using first improvement to avoid long narrow shape as much as possible. In picture, we can see the partition is pretty good. Seems there are still some narrow shapes, but this is mainly because the ocean area will contain much less measurements. The result is pretty satisfacotry."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__\n",
      "import pickle\n",
      "import gzip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weighted_stations = pickle.load(gzip.open( 'weighted_stations.pklz', \"rb\" ))\n",
      "weighted_stations.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'weighted_stations.pklz'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-11f6bfb9c912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweighted_stations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'weighted_stations.pklz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mweighted_stations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferedIOBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__builtin__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m# Issue #13781: os.fdopen() creates a fileobj with a bogus name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'weighted_stations.pklz'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run hierarchical_indexing.py\n",
      "%run misc.py\n",
      "%run merging.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hi.get_prelim_partition().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hi.plot_prelim_partitioning()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hi.partition_plus()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = hi._rectangles\n",
      "r[r['outer_id']==33]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hi.plot_map_plus()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Section 3**. K-means clustering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Section 4**. Comparesion"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**Section 5**. Investigate PCA model on the Partition from K\u00f6ppen climate classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}