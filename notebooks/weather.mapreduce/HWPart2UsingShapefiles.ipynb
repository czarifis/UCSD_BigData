{
 "metadata": {
  "name": "",
  "signature": "sha256:920d508d630a9f5e9de451c8e55b43389c7f7ea0e08b02aaabbf7fdc6b613f41"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://matplotlib.org/basemap/users/merc.html\n",
      "\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.collections import LineCollection\n",
      "from matplotlib import cm\n",
      "import shapefile\n",
      "\n",
      "\n",
      "# llcrnrlat,llcrnrlon,urcrnrlat,urcrnrlon\n",
      "# are the lat/lon values of the lower left and upper right corners\n",
      "# of the map.\n",
      "# lat_ts is the latitude of true scale.\n",
      "# resolution = 'i' means use intermediate resolution coastlines.\n",
      "lonmin=-180;lonmax=180;latsmin=-80;latsmax=80;\n",
      "plt.figure(figsize=(15,10),dpi=300)\n",
      "m = Basemap(projection='merc',llcrnrlat=latsmin,urcrnrlat=latsmax,\\\n",
      "            llcrnrlon=lonmin,urcrnrlon=lonmax,lat_ts=20,resolution='i')\n",
      "m.drawcoastlines()\n",
      "m.fillcontinents(color='coral',lake_color='aqua')\n",
      "ax = plt.subplot(111)\n",
      "# draw parallels and meridians.\n",
      "parallels = np.arange(-80,81,10.)\n",
      "# labels = [left,right,top,bottom]\n",
      "m.drawparallels(parallels,labels=[False,True,True,False])\n",
      "meridians = np.arange(10.,351.,20.)\n",
      "m.drawmeridians(meridians,labels=[True,False,False,True])\n",
      "\n",
      "#m.drawparallels(np.arange(-90.,91.,30.))\n",
      "#m.drawmeridians(np.arange(-180.,181.,60.))\n",
      "m.drawmapboundary(fill_color='aqua')\n",
      "\n",
      "r = shapefile.Reader(r\"/home/ubuntu/shapefiles/gadm2\")\n",
      "shapes = r.shapes()\n",
      "records = r.records()\n",
      "\n",
      "for record, shape in zip(records,shapes):\n",
      "    lons,lats = zip(*shape.points)\n",
      "    data = np.array(m(lons, lats)).T\n",
      "\n",
      "    if len(shape.parts) == 1:\n",
      "        segs = [data,]\n",
      "    else:\n",
      "        segs = []\n",
      "        for i in range(1,len(shape.parts)):\n",
      "            index = shape.parts[i-1]\n",
      "            index2 = shape.parts[i]\n",
      "            segs.append(data[index:index2])\n",
      "        segs.append(data[index2:])\n",
      "\n",
      "    lines = LineCollection(segs,antialiaseds=(1,))\n",
      "    lines.set_facecolors(cm.jet(np.random.rand(1)))\n",
      "    lines.set_edgecolors('r')\n",
      "    lines.set_linewidth(1)\n",
      "    ax.add_collection(lines)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "plt.title('weather stations')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using Geospatial data to create the groups with the base-stations\n",
      "\n",
      "We tried another way to partition the earth using the actual borders/shorelines which we got from this site http://www.gadm.org/ as shape files. These files are special because they use the WGS84, which is a standard World Geodetic System (it's the most recent and most widely used as well.). \n",
      "\n",
      "We also used this library https://code.google.com/p/pyshp/ to process the polygons included in the shapefiles.\n",
      "\n",
      "\n",
      "shp2pgsql -s 26910 -W \"latin1\" GRC_adm2.shp greece gisdatabase > greece.sql\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.basemap import Basemap\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.collections import LineCollection\n",
      "from matplotlib import cm\n",
      "import shapefile\n",
      "\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/gadm2\")\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/Greece/GRC_adm2\")\n",
      "r = shapefile.Reader(r\"/home/ubuntu/shapefiles/USA/USA_adm1\")\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/globalv1/gadm1_lev0\")\n",
      "shapes = r.shapes()\n",
      "fields = r.fields\n",
      "print fields\n",
      "\n",
      "AllGeoms = []\n",
      "records = r.records()\n",
      "#shape(shapes)\n",
      "print len(zip(records,shapes)), 'polygons loaded!'\n",
      "for record, shapez in zip(records,shapes):\n",
      "    lons,lats = zip(*shapez.points)\n",
      "    print np.shape(lons), np.shape(lats)\n",
      "    \n",
      "    kk = zip(lats,lons)\n",
      "    AllGeoms.append(kk)\n",
      "    #print np.shape(kk)\n",
      "    \n",
      "    #print kk\n",
      "    #data = np.array(m(lons, lats)).T\n",
      "#     data = np.array(lons, lats).T\n",
      "    \n",
      "#     print data\n",
      "#     if len(shape.parts) == 1:\n",
      "#         segs = [data,]\n",
      "#     else:\n",
      "#         segs = []\n",
      "#         for i in range(1,len(shape.parts)):\n",
      "#             index = shape.parts[i-1]\n",
      "#             index2 = shape.parts[i]\n",
      "#             segs.append(data[index:index2])\n",
      "#         segs.append(data[index2:])\n",
      "\n",
      "#     lines = LineCollection(segs,antialiaseds=(1,))\n",
      "#     lines.set_facecolors(cm.jet(np.random.rand(1)))\n",
      "#     lines.set_edgecolors('r')\n",
      "#     lines.set_linewidth(1)\n",
      "#     ax.add_collection(lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('DeletionFlag', 'C', 1, 0), ['ID_0', 'N', 9, 0], ['ISO', 'C', 3, 0], ['NAME_0', 'C', 75, 0], ['ID_1', 'N', 9, 0], ['NAME_1', 'C', 75, 0], ['NL_NAME_1', 'C', 50, 0], ['VARNAME_1', 'C', 150, 0], ['TYPE_1', 'C', 50, 0], ['ENGTYPE_1', 'C', 50, 0]]\n",
        "51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " polygons loaded!\n",
        "(2188,) (2188,)\n",
        "(4016,) (4016,)\n",
        "(1200,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (1200,)\n",
        "(1025,) (1025,)\n",
        "(5407,) (5407,)\n",
        "(31354,) (31354,)\n",
        "(434,) (434,)\n",
        "(40636,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (40636,)\n",
        "(100006,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (100006,)\n",
        "(5191,) (5191,)\n",
        "(2028,) (2028,)\n",
        "(1505,) (1505,)\n",
        "(19994,) (19994,)\n",
        "(1535,) (1535,)\n",
        "(2966,) (2966,)\n",
        "(40527,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (40527,)\n",
        "(1079,) (1079,)\n",
        "(1468,) (1468,)\n",
        "(100385,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (100385,)\n",
        "(552,) (552,)\n",
        "(2334,) (2334,)\n",
        "(78088,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (78088,)\n",
        "(75916,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (75916,)\n",
        "(4439,) (4439,)\n",
        "(2552,) (2552,)\n",
        "(421,) (421,)\n",
        "(11029,) (11029,)\n",
        "(612881,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (612881,)\n",
        "(1655,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (1655,)\n",
        "(1667,) (1667,)\n",
        "(76020,) (76020,)\n",
        "(327,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (327,)\n",
        "(11785,) (11785,)\n",
        "(7146,) (7146,)\n",
        "(163,) (163,)\n",
        "(270629,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (270629,)\n",
        "(27205,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (27205,)\n",
        "(28665,) (28665,)\n",
        "(4380,) (4380,)\n",
        "(3082,) (3082,)\n",
        "(1969,) (1969,)\n",
        "(1834,) (1834,)\n",
        "(514,) (514,)\n",
        "(2292,) (2292,)\n",
        "(113927,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (113927,)\n",
        "(84488,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (84488,)\n",
        "(93876,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (93876,)\n",
        "(38046,) (38046,)\n",
        "(1991,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (1991,)\n",
        "(5933,) (5933,)\n",
        "(11411,) (11411,)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def in_hull(p, hull):\n",
      "    \"\"\"\n",
      "    Test if points in `p` are in `hull`\n",
      "\n",
      "    `p` should be a `NxK` coordinates of `N` points in `K` dimension\n",
      "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
      "    coordinates of `M` points in `K`dimension for which a Delaunay triangulation\n",
      "    will be computed\n",
      "    \"\"\"\n",
      "    from scipy.spatial import Delaunay\n",
      "    if not isinstance(hull,Delaunay):\n",
      "        hull = Delaunay(hull)\n",
      "\n",
      "    return hull.find_simplex(p)>=0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = [[35.435096, 23.768364]] ## That's in Greece\n",
      "#Y = [[32.975025, -116.861099]]   ## That's in California\n",
      "hullCounter=0\n",
      "for hull in AllGeoms:\n",
      "\n",
      "    result = in_hull(Y,hull)\n",
      "    #print result\n",
      "    if result==True:\n",
      "        \n",
      "        print hullCounter\n",
      "        break\n",
      "    hullCounter+=1\n",
      "    \n",
      "#plot_in_hull(Y,X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Let's load the data from the csv file we created on the last part..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header_row=['station','maxcount','mincount'] # note the header has duplicate column values\n",
      "maxmindf = pd.read_csv('basestationmaxmin.csv', names=header_row)\n",
      "maxmindf.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>station</th>\n",
        "      <th>maxcount</th>\n",
        "      <th>mincount</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> AG000060390</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> AG000060611</td>\n",
        "      <td> 730</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> AG000060680</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> AGE00135039</td>\n",
        "      <td>  26</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> AJ000037575</td>\n",
        "      <td> 209</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "       station  maxcount  mincount\n",
        "0  AG000060390       365         0\n",
        "1  AG000060611       730         0\n",
        "2  AG000060680       365         0\n",
        "3  AGE00135039        26         0\n",
        "4  AJ000037575       209         0\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!gunzip stations.pkl.gz\n",
      "import pickle\n",
      "!gunzip -c stations.pkl.gz > stations.pkl  # This command also keeps the initial gz file....\n",
      "stations=pickle.load(open('stations.pkl', 'rb'))\n",
      "df = pd.read_pickle('stations.pkl')\n",
      "df['station'] = df.index # let's create a column with the indexes (base-station names)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "      <th>station</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>ACW00011604</th>\n",
        "      <td> 17.1167</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   10.1</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ST JOHNS COOLIDGE FLD</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> ACW00011604</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ACW00011647</th>\n",
        "      <td> 17.1333</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   19.2</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              ST JOHNS</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> ACW00011647</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AE000041196</th>\n",
        "      <td> 25.3330</td>\n",
        "      <td> 55.5170</td>\n",
        "      <td>   34.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   SHARJAH INTER. AIRP</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 41196</td>\n",
        "      <td> AE000041196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AF000040930</th>\n",
        "      <td> 35.3170</td>\n",
        "      <td> 69.0170</td>\n",
        "      <td> 3366.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>          NORTH-SALANG</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 40930</td>\n",
        "      <td> AF000040930</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AG000060390</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>    ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "      <td> AG000060390</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 9 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "             latitude  longitude  elevation state                   name  \\\n",
        "ACW00011604   17.1167   -61.7833       10.1   NaN  ST JOHNS COOLIDGE FLD   \n",
        "ACW00011647   17.1333   -61.7833       19.2   NaN               ST JOHNS   \n",
        "AE000041196   25.3330    55.5170       34.0   NaN    SHARJAH INTER. AIRP   \n",
        "AF000040930   35.3170    69.0170     3366.0   NaN           NORTH-SALANG   \n",
        "AG000060390   36.7167     3.2500       24.0   NaN     ALGER-DAR EL BEIDA   \n",
        "\n",
        "            GSNFLAG HCNFLAG  WMOID      station  \n",
        "ACW00011604     NaN     NaN    NaN  ACW00011604  \n",
        "ACW00011647     NaN     NaN    NaN  ACW00011647  \n",
        "AE000041196     GSN     NaN  41196  AE000041196  \n",
        "AF000040930     GSN     NaN  40930  AF000040930  \n",
        "AG000060390     GSN     NaN  60390  AG000060390  \n",
        "\n",
        "[5 rows x 9 columns]"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = pd.merge(df, maxmindf, on='station', how='inner')\n",
      "res.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "      <th>station</th>\n",
        "      <th>maxcount</th>\n",
        "      <th>mincount</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24</td>\n",
        "      <td> NaN</td>\n",
        "      <td>     ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "      <td> AG000060390</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 28.0500</td>\n",
        "      <td>  9.6331</td>\n",
        "      <td>  561</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              IN-AMENAS</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60611</td>\n",
        "      <td> AG000060611</td>\n",
        "      <td> 730</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 22.8000</td>\n",
        "      <td>  5.4331</td>\n",
        "      <td> 1362</td>\n",
        "      <td> NaN</td>\n",
        "      <td>            TAMANRASSET</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60680</td>\n",
        "      <td> AG000060680</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 35.7297</td>\n",
        "      <td>  0.6500</td>\n",
        "      <td>   50</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ORAN-HOPITAL MILITAIRE</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> AGE00135039</td>\n",
        "      <td>  26</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 41.5500</td>\n",
        "      <td> 46.6670</td>\n",
        "      <td>  490</td>\n",
        "      <td> NaN</td>\n",
        "      <td>               ZAKATALA</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 37575</td>\n",
        "      <td> AJ000037575</td>\n",
        "      <td> 209</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 11 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "   latitude  longitude  elevation state                    name GSNFLAG  \\\n",
        "0   36.7167     3.2500         24   NaN      ALGER-DAR EL BEIDA     GSN   \n",
        "1   28.0500     9.6331        561   NaN               IN-AMENAS     GSN   \n",
        "2   22.8000     5.4331       1362   NaN             TAMANRASSET     GSN   \n",
        "3   35.7297     0.6500         50   NaN  ORAN-HOPITAL MILITAIRE     NaN   \n",
        "4   41.5500    46.6670        490   NaN                ZAKATALA     NaN   \n",
        "\n",
        "  HCNFLAG  WMOID      station  maxcount  mincount  \n",
        "0     NaN  60390  AG000060390       365         0  \n",
        "1     NaN  60611  AG000060611       730         0  \n",
        "2     NaN  60680  AG000060680       365         0  \n",
        "3     NaN    NaN  AGE00135039        26         0  \n",
        "4     NaN  37575  AJ000037575       209         0  \n",
        "\n",
        "[5 rows x 11 columns]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print df[['latitude']].tolist()\n",
      "bsCoords = zip(res[\"latitude\"].tolist(),df[\"longitude\"].tolist())\n",
      "stationNames = res[\"station\"].tolist()\n",
      "#bslongs = df[\"longitude\"].tolist()\n",
      "#print bsCoords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for hull in AllGeoms:\n",
      "    ftz = in_hull(bsCoords,kk)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices = []\n",
      "cntr = 0\n",
      "for ft in ftz:\n",
      "    if ft==True:\n",
      "        print 'paketo'\n",
      "        indices.append(cntr)\n",
      "    cntr+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n",
        "paketo\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[39696, 39916, 39991, 40034, 40137, 47932, 47959, 48007, 48008, 48009, 48010, 48011, 48013, 48015, 48022, 48026, 49786, 49787, 49788, 49789, 49790, 49791, 49792, 49793, 49794, 49795, 49796, 49797, 49798, 49799, 49800, 49801, 49802, 49803, 49804, 49805, 49806, 49807, 49808, 49809, 49810, 49811, 49812, 49813, 49814, 49815, 49816, 49817, 49818, 49819, 49820, 49821, 49822, 49823, 49824, 49825, 49826, 49827, 49828, 49829, 49830, 49831, 49832, 49833, 49834, 49835, 49836, 49837, 49838, 49839, 49840, 49841, 49842, 49843, 49844, 49845, 49846, 49847, 49848, 49849, 49850, 49851, 49852, 49853, 49854, 49855, 49856, 49857, 49858, 49859, 49860, 49861, 49862, 49863, 49864, 49865, 49866, 49867, 49868, 49869, 49870, 49871, 49872, 49873, 49874, 49875, 49876, 49877, 49878, 49879, 49880, 49881, 49882, 49883, 49884, 49885, 49886, 49887, 49888, 49889, 49890, 49891, 49892, 49893, 49894, 49895, 49896, 49897, 49898, 49899, 49900, 49901, 49902, 49903, 49904, 49905, 49906, 49907, 49908, 49909, 49910, 49911, 49912, 49913, 49914, 49915, 49916, 49917, 49918, 49919, 49920, 49921, 49922, 49923, 49924, 49925, 49926, 49927, 49928, 49929, 49930, 49931, 49932, 49933, 49934, 49935, 49936, 49937, 49938, 49939, 49940, 49941, 49942, 49943, 49944, 49945, 49946, 49947, 49948, 49949, 49950, 49951, 49952, 49953, 49954, 49955, 49956, 49957, 49958, 49959, 49960, 49961, 49962, 49963, 49964, 49965, 49966, 49967, 49968, 49969, 49970, 49971, 49972, 49973, 49974, 49975, 49976, 49977, 49978, 49979, 49980, 49981, 49982, 49983, 49984, 49985, 58574, 58606, 58635, 58644, 58648, 58693, 58762, 58767, 58807, 58808, 58820, 58822, 59400, 59502, 59544, 59571, 64929, 64930, 64931, 64937, 64939, 64968, 64969, 64999, 65000, 65001, 65002, 65016, 65017, 65050, 65059, 65060, 65061, 65064, 65065, 65066, 65067, 65068, 65069, 65088, 65096, 65122, 65140, 65153, 65155, 65164, 65205, 65206, 65227, 65232, 65242, 65243, 65244, 65246, 65273, 65281, 65290, 65297, 65310, 65311, 65312, 65313, 65327, 65328, 65329, 65330, 65333, 65337, 65343, 65344, 65366, 67055, 67056, 67057, 67058, 67059, 67060, 67061, 67062, 67063, 67064, 67065, 67066, 67068, 67069, 67070, 67071, 67072, 67073, 67074, 67075, 67076, 67077, 67078, 67079, 67080, 67081, 67082, 67083, 67084, 67085, 67086, 67087, 67088, 67089, 67090, 67091, 67092, 67093, 67094, 67095, 67096, 67097, 67098, 67099, 67100, 67101, 67102, 67103, 67104, 67105, 67106, 67107, 67108, 67109, 67110, 67111, 67112, 67113, 67114, 67115, 67116, 67117, 67118, 67119, 67120, 67121, 67122, 67123, 67124, 67125, 67126, 67127, 67128, 67129, 67130, 67131, 67132, 67133, 67134, 67135, 67136, 67137, 67138, 67139, 67140, 67141, 67142, 67143, 67144, 67145, 67146, 67147, 67148, 67149, 67150, 67151, 67152, 67153, 67154, 67155, 67156, 67157, 67158, 67159, 67160, 67161, 67162, 67163, 67164, 67165, 67166, 67167, 67168, 67169, 67170, 67171, 67172, 67173, 67174, 67175, 67176, 67177, 67178, 67179, 67180, 67181, 67182, 67183, 67184, 67185, 67186, 67187, 67188, 67189, 67190, 67191, 67192, 67193, 67194, 67195, 67196, 67197, 67198, 67199, 67200, 67201, 67202, 67203, 67204, 67205, 67206, 67207, 67208, 67209, 67210, 67211, 67212, 67213, 67214, 67215, 67216, 67217, 67218, 67219, 67220, 67221, 67222, 67223, 67224, 67225, 67226, 67227, 67228, 67229, 67230, 67231, 67232, 67233, 67234, 67235, 67236, 67237, 67238, 67239, 67240, 67241, 67242, 67243, 67244, 67245, 67246, 67247, 67248, 67249, 67250, 67251, 67252, 67253, 67254, 67255, 67256, 67257, 67258, 67259, 67260, 67261, 67262, 67263, 67264, 67265, 67266, 67267, 67268, 67269, 67270, 67271, 67272, 67273, 67274, 67275, 67276, 67277, 67278, 67279, 67280, 67281, 67282, 67283, 67284, 67285, 67286, 67287, 67288, 67289, 67290, 67291, 67292, 67293, 67294, 67295, 67296, 67297, 67298, 67299, 67300, 67301, 67302, 67303, 67304, 67305, 67306, 67307, 67308, 67309, 67310, 67311, 67312, 67313, 67314, 67315, 67316, 67317, 67318, 67319, 67320, 67321, 67322, 67323, 67324, 67325, 67326, 67327, 67328, 67329, 67330, 67331, 67332, 67333, 67334, 67335, 67336, 67337, 67338, 67339, 67340, 67341, 67342, 67343, 67344, 67345, 67346, 67347, 67348, 67349, 67350, 67351, 67352, 67353, 67354, 67355, 67356, 67357, 67358, 67359, 67360, 67361, 67362, 67363, 67364, 81526, 81530, 81542, 81554, 81564, 81586, 81588, 81615, 81643, 81648, 81678, 81685, 82942, 82978, 83004, 83239, 83243, 83246, 83247, 83263, 83305, 83311, 83318, 83339, 83340, 83716, 84046, 84052, 84059, 84154, 84359, 84360, 84375, 84376, 84398]\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print [bsCoords[29423]]\n",
      "print stationNames[29423]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(39.649999999999999, 22.449999999999999)]\n",
        "GR000016648\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fields"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('DeletionFlag', 'C', 1, 0), ['ID_0', 'N', 9, 0], ['ISO', 'C', 3, 0], ['NAME_0', 'C', 75, 0], ['ID_1', 'N', 9, 0], ['NAME_1', 'C', 75, 0], ['NL_NAME_1', 'C', 50, 0], ['VARNAME_1', 'C', 150, 0], ['TYPE_1', 'C', 50, 0], ['ENGTYPE_1', 'C', 50, 0]]\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print records[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 'AFG', 'Afghanistan', 'AFGHANISTAN', 'Afghanistan', 'Afghanestan', '                                                                                                                                                      ', '                                                                                                                                                                ', '                                                  ', 'Afghanistan', 'Afganist\\xe1n', '??????????', '?????????', '???', '                                                                                                    ', '                                                  ', 'Afghanistan', 'AF', '  ', 'AF', '4.00000000000e+000', '~1900', 'Present', '1.00000000000e+000', '2.17649550000e+007', '6.41869188000e+005', '3.39087082024e+001', 'Southern Asia', 'Asia', '1.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', 'South Asia', 'Low income', 'Debt not classified', '                                                                                                                                                                                                                                                              ', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '1.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '1.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '0.00000000000e+000', '1.00000000000e+000', '5.71046465792e+001', '6.27495882997e+001']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Country = records[84107][2]\n",
      "# for i in range(len(records[84151])):\n",
      "#     print records[84151][i],'---',i\n",
      "    \n",
      "print records[84107][3], records[84107][5]\n",
      "print records[84151][3], records[84151][5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Greece Stere\ufffd Ell\ufffdda\n",
        "Greece Kriti\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print records[96][28], records[96][3], records[96][29]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Southern Europe Greece Europe\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import collections\n",
      "d1 = collections.defaultdict(list)\n",
      "d2 = collections.defaultdict(list)\n",
      "d3 = collections.defaultdict(list)\n",
      "for jj in range(len(bsCoords)):\n",
      "    Y = [bsCoords[jj]]\n",
      "    hullCounter=0\n",
      "    for hull in AllGeoms:\n",
      "\n",
      "        result = in_hull(Y,hull)\n",
      "        #print result\n",
      "        if result==True:\n",
      "            general = records[hullCounter][28] \n",
      "            specific = records[hullCounter][3]\n",
      "            moregeneral = records[hullCounter][29] \n",
      "            d1[stationNames[jj]].append(specific)\n",
      "            d2[stationNames[jj]].append(general)\n",
      "            d3[stationNames[jj]].append(moregeneral)\n",
      "            #print hullCounter\n",
      "            break\n",
      "        hullCounter+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-7e229ed780ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhull\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAllGeoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_hull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-20-ecd4a16e3798>\u001b[0m in \u001b[0;36min_hull\u001b[1;34m(p, hull)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDelaunay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDelaunay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mhull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDelaunay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_simplex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay.__init__ (scipy/spatial/qhull.c:14197)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser.__init__ (scipy/spatial/qhull.c:12816)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay._update (scipy/spatial/qhull.c:14575)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser._update (scipy/spatial/qhull.c:13306)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     17\u001b[0m                             out=out, keepdims=keepdims)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     return um.minimum.reduce(a, axis=axis,\n\u001b[0;32m     21\u001b[0m                             out=out, keepdims=keepdims)\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "with open('geogrouping.json', 'w') as outfile:\n",
      "  json.dump(d, outfile)\n",
      "    \n",
      "import pickle\n",
      "with open('geogrouping.pkl', 'w') as outfile:\n",
      "  pickle.dump(d, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#need to check if the file is infact correct... \n",
      "!head -c 300 geogrouping.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Great let's build the map/reduce job that computes the PCA for each group!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile pcatogroups.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of measurements for each base-station\n",
      "\"\"\"\n",
      "import sys\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.step import MRStep\n",
      "import re\n",
      "from sys import stderr\n",
      "import json\n",
      "\n",
      "class SqliteJob(MRJob):\n",
      "    \n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(SqliteJob, self).__init__(*args, **kwargs)\n",
      "        self.data = {}\n",
      "        \n",
      "    \n",
      "\n",
      "    def configure_options(self):\n",
      "        super(SqliteJob, self).configure_options()\n",
      "        self.add_file_option('--hashmap')\n",
      "\n",
      "    def mapper_init(self):\n",
      "        json_data=open(self.options.hashmap)\n",
      "\n",
      "        self.data = json.load(json_data)\n",
      "        \n",
      "        #sys.stderr.write(str(data))\n",
      "        json_data.close()\n",
      "        # make dictionary available to mapper\n",
      "        #sys.stderr.write('MAPPER_INIT!!')\n",
      "        sys.stderr.write(str(self.options.hashmap))\n",
      "        \n",
      "        #self.sqlite_conn = sqlite3.connect(self.options.database)\n",
      "    def mapper(self, _, line):\n",
      "#         for key in self.data:\n",
      "#             sys.stderr.write( key + ' corresponds to'+ str(self.data[key])+'\\n')\n",
      "\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                \n",
      "                # if the header is spotted add it, it's not going to matter\n",
      "                # since no base-station with name: 'header' exists, so when we do the\n",
      "                # inner join this will be pruned\n",
      "                \n",
      "                out=('header','0~0')\n",
      "                yield out\n",
      "            else:\n",
      "                if elements[1]=='TMAX':\n",
      "                    \n",
      "                    # Uncomment that to get measurements from a specific year.\n",
      "                    #if elements[2]=='1973':\n",
      "                        \n",
      "                        # Use the following instead if you want to avoid null values\n",
      "                        #out=(str(self.data[elements[0]]),str(filter(None,elements[3:])))#+'~'+str(0))\n",
      "                        \n",
      "                        out=(str(self.data[elements[0]]),(elements[3:]))#+'~'+str(0))\n",
      "                        yield out\n",
      "                #elif elements[1]=='TMIN':\n",
      "                    \n",
      "                    # Uncomment that to get measurements from a specific year.\n",
      "                    #if elements[2]=='1973':\n",
      "                        \n",
      "               #         out=(elements[0],str(0)+'~'+str(len(filter(None, elements[3:]))))\n",
      "               #         yield out\n",
      "        except Exception, e:\n",
      "            #stderr.write('Error in line:\\n'+line)\n",
      "            #stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            \n",
      "            # Again... This will be pruned when we do the inner join later...\n",
      "            \n",
      "            out=('error','0~0')\n",
      "            yield out\n",
      "\n",
      "        \n",
      "#     def combiner(self, station, vals):\n",
      "#         yield (0,0)\n",
      "        \n",
      "    def reducer(self, group, vals):\n",
      "\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        sys.stderr.write('group range is: '+group+'\\n')\n",
      "        valArr = []\n",
      "        \n",
      "        for val in vals:\n",
      "            #sys.stderr.write(str(val)+'\\n')\n",
      "            valArr.append(val)\n",
      "\n",
      "\n",
      "\n",
      "        # Awesome! Let's use a Dataframe!\n",
      "        ddtf = pd.DataFrame(valArr,columns = range(1,366))\n",
      "        \n",
      "        # Since we have a lot of empty values replace them with np.nan \n",
      "        # this is gonna make the processing easier\n",
      "        \n",
      "        ddtf = ddtf.replace('', np.nan)\n",
      "        \n",
      "        # Let's check out the measurements\n",
      "        #sys.stderr.write(str(ddtf.head()))  # Checked!\n",
      "        \n",
      "\n",
      "        \n",
      "        # switch the type of every element in the dataframe from an object to a float\n",
      "        for listElem in list(ddtf.columns.values):\n",
      "            #sys.stderr.write(str(listElem))\n",
      "            ddtf[listElem] = ddtf[listElem].astype(float)#.fillna(0.0)\n",
      "        \n",
      "        \n",
      "        # It should be float now\n",
      "        # sys.stderr.write('\\nddtf types: '+str(ddtf.dtypes))   # Yes it is!\n",
      "\n",
      "        \n",
      "        M=ddtf.loc[:,:].transpose()\n",
      "        \n",
      "        # check the shape of M\n",
      "        #sys.stderr.write('\\nM shape is: '+str(M.shape))\n",
      "        \n",
      "        M=M.dropna(axis=1)\n",
      "        \n",
      "        # check again after dropping the nan values\n",
      "        sys.stderr.write('\\n M shape after droping NaNs is: '+str(np.shape(M)))\n",
      "        \n",
      "        (columns,rows)=np.shape(M)\n",
      "        \n",
      "        # check if there are any values in this group\n",
      "        \n",
      "        \n",
      "        # Let's compute the covariance, (We could use np.cov instead...)\n",
      "        if rows !=0:\n",
      "            Mean=np.mean(M, axis=1).values\n",
      "\n",
      "\n",
      "            C=np.zeros([columns,columns])   # Sum\n",
      "            N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
      "\n",
      "            for i in range(rows):\n",
      "                row=M.iloc[:,i]-Mean;\n",
      "                outer=np.outer(row,row)\n",
      "                valid=np.isnan(outer)==False\n",
      "                C[valid]=C[valid]+outer[valid]  # update C with the valid location in outer\n",
      "                N[valid]=N[valid]+1\n",
      "            valid_outer=np.multiply(1-np.isnan(N),N>0)\n",
      "            cov=np.divide(C,N)\n",
      "            cov2 = np.cov(M)\n",
      "            sys.stderr.write('\\ncov is: \\n'+str(cov)+'\\ncov2 is: \\n'+str(cov2))\n",
      "#             for elem in cov:\n",
      "#                 sys.stderr.write('\\ncov is: \\n'+str(elem))\n",
      "            sys.stderr.write('\\nnp.sum(cov) : '+str(np.sum(cov)))\n",
      "            summation = np.sum(cov)\n",
      "            if summation!=0:\n",
      "                U,D,V=np.linalg.svd(cov)\n",
      "                sys.stderr.write('\\nyielding D')\n",
      "                yield (group+str(rows),str(D))\n",
      "\n",
      "                \n",
      "        else:\n",
      "            sys.stderr.write('nothing to do here')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    SqliteJob.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}